{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dry Part"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Question 1\n",
    "\n",
    "We would like to prove that $ \\epsilon_t = \\sum_{i=1}^{N} D_i^{t+1}\\cdot\\mathbb{1}_{h_t(x_i)\\neq y_i} = \\frac{1}{2} $ for $h_t$ the chosen weak classifier.  \n",
    "\n",
    "Recall our definitions from the lecture:  \n",
    "$$\n",
    "D_i^{(t+1)}=\\frac{D_i^{(t)}\\cdot exp(-w_ty_ih_t(x_i))}{\\sum_{j=1}^N D_j^{(t)}\\cdot exp(-w_ty_ih_t(x_j))} \\\\\n",
    "w_t=\\frac{1}{2}log(\\frac{1}{\\epsilon_t}-1)\n",
    "$$\n",
    "\n",
    "We will later on use the following statement:\n",
    "\n",
    "$$\n",
    "exp(-2w_t) = exp(-2 \\cdot \\frac{1}{2}log(\\frac{1}{\\epsilon_t}-1)) = \\frac{\\epsilon_t}{1-\\epsilon_t}\n",
    "$$\n",
    "\n",
    "For ease of use, we shall denote the set of indices of the examples which where classfied wrongly as $A$, and $A^C$ for the examples which where classified correctly.  \n",
    "Let's massage the target mathematical definition for the error:  \n",
    "$$\n",
    " \\epsilon_t = \\sum_{i=1}^{N} D_i^{t+1}\\cdot\\mathbb{1}_{h_t(x_i)\\neq y_i} =\\\\\n",
    " \\sum_{i\\in A} D_i^{t+1}\\cdot\\mathbb{1} + \\sum_{i\\in A^C} D_i^{t+1}\\cdot\\mathbb{0} = \\\\\n",
    "$$\n",
    "We can cancel out the correct samples as they contribute 0 to the error. Then we can plug in our expression for $D_i^{t+1}$:\n",
    "$$\n",
    " \\sum_{i\\in A} D_i^{t+1} =\\\\\n",
    " \\sum_{i\\in A} \\frac{D_i^{(t)}\\cdot exp(-w_ty_ih_t(x_i))}{\\sum_{j=1}^N D_j^{(t)}\\cdot exp(-w_ty_ih_t(x_j))} =\\\\\n",
    "$$\n",
    "Notice how $y_ih_t(x_i) = -1$ for every wrong sample, simplifyig our expression:  \n",
    "$$\n",
    " \\sum_{i\\in A} \\frac{D_i^{(t)}\\cdot exp(-w_t\\cdot(-1))}{\\sum_{j=1}^N D_j^{(t)}\\cdot exp(-w_ty_ih_t(x_j))} =\\\\\n",
    " \\sum_{i\\in A} \\frac{D_i^{(t)}\\cdot exp(w_t)}{\\sum_{j=1}^N D_j^{(t)}\\cdot exp(-w_ty_ih_t(x_j))} =\\\\\n",
    " $$\n",
    " We can now seperate the sum in the denominator to two parts - the wrongly classfied samples and the correct ones, and treat the expressions similarly to before:\n",
    " $$\n",
    " \\frac{ \\sum_{i\\in A} D_i^{(t)}\\cdot exp(w_t)}{\\sum_{j\\in A} D_j^{(t)}\\cdot exp(-w_ty_ih_t(x_j)) + \\sum_{j\\in A^C} D_j^{(t)}\\cdot exp(-w_ty_ih_t(x_j))} =\\\\\n",
    "\\frac{ \\sum_{i\\in A} D_i^{(t)}\\cdot exp(w_t)}{\\sum_{j\\in A} D_j^{(t)}\\cdot exp(w_t) + \\sum_{j\\in A^C} D_j^{(t)}\\cdot exp(-w_t)} =\\\\\n",
    " \\frac{\\sum_{i\\in A} D_i^{(t)}}{\\sum_{j\\in A} D_j^{(t)} + \\sum_{j\\in A^C} D_j^{(t)}\\cdot exp(-2w_t)} =\\\\\n",
    "$$\n",
    "Finally, we managed to get to the given expression we needed, now we just need to plug in our calculation of $exp(-2w_t)$:\n",
    "$$\n",
    "\\frac{\\epsilon_t}{\\epsilon_t + (1-\\epsilon_t)\\cdot exp(-2w_t)} =\\\\\n",
    "\\frac{\\epsilon_t}{\\epsilon_t + (1-\\epsilon_t)\\cdot \\frac{\\epsilon_t}{1-\\epsilon_t}} =\\\\\n",
    "\\frac{\\epsilon_t}{\\epsilon_t + \\epsilon_t }=\\\\\n",
    "\\frac{1}{2}\n",
    " $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2\n",
    "\n",
    "### 2.1\n",
    "We shall show that $F_{\\alpha\\Theta}(x)=c\\cdot F_{\\Theta}(x)$ where $c=\\alpha^L$ for all $L>1$.\n",
    "We proove this using induction over $L$.\n",
    "\n",
    "**Base:** For $L=2$ (A nueral network must be with at least 2 layers).\n",
    "$$\n",
    "F_{\\alpha\\Theta}(x)=\\alpha {W^{(2)}}^T\\cdot h_{\\alpha\\Theta}^{(1)}(x)=\\\\\n",
    "\\alpha W^{(2)}\\cdot\\sigma(\\alpha {W^{(1)}}^T x)=\\\\\n",
    "\\alpha^2 W^{(2)}\\cdot\\sigma( {W^{(1)}}^T x)=\\\\\n",
    "\\alpha^2 W^{(2)}\\cdot h_{\\Theta}^{(1)}(x)=\\\\\n",
    "\\alpha^2 F_{\\Theta}(x)\n",
    "$$\n",
    "\n",
    "**Step:** We assume that $F_{\\alpha\\Theta}(x)=\\alpha^L\\cdot F_{\\Theta}(x)$ for all $L\\le k$ for some $k\\in\\mathbb{N}$.\n",
    "Therfore for $L=k+1$ we can show that:  \n",
    "$$\n",
    "F_{\\alpha\\Theta}(x) = \\alpha {W^{(k+1)}}^T\\cdot h_{\\alpha\\Theta}^{(k)}(x) =\\\\\n",
    "\\alpha {W^{(k+1)}}^T\\cdot \\sigma(\\alpha {W^{(k)}}^T\\cdot h_{\\alpha\\Theta}^{(k-1)}(x))=\\\\\n",
    "\\alpha {W^{(k+1)}}^T\\cdot \\sigma(\\alpha^k {W^{(k)}}^T\\cdot h_{\\Theta}^{(k-1)}(x))=\\\\\n",
    "\\alpha^{k+1} {W^{(k+1)}}^T\\cdot \\sigma({W^{(k)}}^T\\cdot h_{\\Theta}^{(k-1)}(x))=\\\\\n",
    "\\alpha^{k+1} {W^{(k+1)}}^T\\cdot h_{\\Theta}^{(k)}(x)=\\\\\n",
    "\\alpha^{k+1} F_{\\Theta}(x)\n",
    "$$\n",
    "\n",
    "### 2.2\n",
    "Let's look on the softmax expression for one target label:\n",
    "$$\n",
    "C_{y_i} = \\frac{exp(\\alpha^L \\cdot (F_\\Theta(x))_i)}{\\sum_{j=1}^{K} exp(\\alpha^L \\cdot (F_\\Theta(x))_j)}\\\\\n",
    "$$\n",
    "We can try taking the logarithm to simplfy the term:  \n",
    "$$\n",
    "log(C_{y_i}) = \\alpha^L \\cdot (F_\\Theta(x))_i) - log(\\sum_{j=1}^{K} exp(\\alpha^L \\cdot (F_\\Theta(x))_j)) \\\\\n",
    "$$\n",
    "Now taking the limit becomes simpler, as many terms either become $0$, or $exp(0)=1$. And we get:\n",
    "$$\n",
    "Lim_{\\alpha \\rightarrow 0} log(C_{y_i}) = -log(K) \\\\\n",
    "Lim_{\\alpha \\rightarrow 0} C_{y_i} = \\frac{1}{K}\n",
    "$$\n",
    "This means that the distribution induced by the softmax function is actually a discrete uniform distribution with K labels.\n",
    "\n",
    "\n",
    "### 2.3\n",
    "Let's now consider again the softmax expression for one category $i$:\n",
    "$$\n",
    "C_{y_i} = \\frac{exp(\\alpha^L \\cdot (F_\\Theta(x))_i)}{\\sum_{j=1}^{K} exp(\\alpha^L \\cdot (F_\\Theta(x))_j)}\\\\\n",
    "$$\n",
    "\n",
    "We now have 2 options, either $i$ is the index of the maximal score of $F_\\Theta(x)$, or it's not.  \n",
    "If it is not the maximal score, then the limit will converge as follows:  \n",
    "\n",
    "$$\n",
    "0 < C_{y_i} = \\frac{exp(\\alpha^L \\cdot (F_\\Theta(x))_i)}{\\sum_{j=1}^{K} exp(\\alpha^L \\cdot (F_\\Theta(x))_j)}\\\\\n",
    "< \\frac{exp(\\alpha^L \\cdot (F_\\Theta(x))_i)}{exp(\\alpha^L \\cdot (F_\\Theta(x))_{max})} \\\\\n",
    "= exp(\\alpha^L \\cdot ((F_\\Theta(x))_i) - (F_\\Theta(x))_{max}))\n",
    "$$\n",
    "\n",
    "\n",
    "But $((F_\\Theta(x))_i) - (F_\\Theta(x))_{max})$ is defintly negative, which means the whole expression converges to $0$ as $\\alpha\\rightarrow\\infty$.  \n",
    "\n",
    "Because of this, the only possible value the $C_{y_i}$ when $i=max$ can get is $1$.\n",
    "\n",
    "Basically, this makes the probability distribution a \"spiky\" one around the maximal score, which kind of makes the whole model \"more decisive\" of what label to give sample $x$.  \n",
    "In contrast, we saw when $\\alpha\\rightarrow 0$ the model is not decisive at all."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyCharm (DL_hw1)",
   "language": "python",
   "name": "pycharm-44a6c8e0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
